{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655003b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the current working directory\n",
    "#os.chdir('/Users/robertocerina/Desktop/CSSci Semester IV/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6126db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51d5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter Data for January 6th of 2020\n",
    "df = pd.read_excel('../../data/test/1000_random_rows_24_07_2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701b18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below follows an example of how to label data using the OpenAI API. \n",
    "# You can do this yourself by setting up an API key on the OpenAI website. \n",
    "# Here I'm just showing this as an example, no need for you to replicate this part. \n",
    "\n",
    "# !pip install openai==0.28.1\n",
    "\n",
    "import openai\n",
    "import time # for error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74d3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API settings for Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://computationalsocialsciences.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "\n",
    "system_prompt = \"You will be asked to label a tweet according to some characteristics. Always answer with the name of the characteristic, follower by the label. E.g. Lexical Diversity: 5. Do not use any other language.\"\n",
    "user_prompt = ('Lexical Diversity: Rate the variety and complexity of the vocabulary used in the tweet on a 7-point Likert scale (1 = very low diversity, 7 = very high diversity).\\n'\n",
    "               'Sentiment Analysis: Categorize the emotional tone of the tweet as either negative, neutral, or positive.\\n'\n",
    "               'Spelling and Grammar Quality: Assess the quality of spelling and grammar in the tweet on a 7-point Likert scale (1 = very poor quality, full of errors; 7 = excellent quality, no errors).\\n'\n",
    "               'Content Originality: Rate the originality of the content on a 7-point Likert scale (1 = completely unoriginal, likely plagiarized; 7 = highly original, unique content).\\n'\n",
    "               'Use of URLs: For URLs included in the tweet, classify the Tweet in one of the following categories: [`URL present + leads to reliable sources`, `URL present + leads to unreliable sources`,`URL present + reliability of sources not attainable`, `URL not present`]\\n'\n",
    "               'Hashtag Usage: Report on the use of hashtags in the tweet as one of the following categories: [`Yes - genuine usage`, `Yes - artificial usage`, `No`]\\n'\n",
    "               'Political Leanings: Provide an assessment on whether the Tweet`s content can be categorised as Liberal, Conservative, Independent or Cannot say.\\n'\n",
    "               'Bot Assessment: Provide an assessment on whether the tweet was likely written by a bot. Answer with Yes or No.\\n'\n",
    "               'Content Accuracy Assessment: Provide an assessment on a 7-point Likert representing the accuracy of the claims in the tweet, (1 = all claims in the tweet are verifiably inaccurate; 7 = all claims are true and verifiable).\\n'\n",
    "               'Conspiracy Assessment: Report on whether the tweet contributes to spread conspiracy theories [`Yes`,`No`,`Cannot say`] .\\n'\n",
    "               'Offensive Assessment: This tweet is offensive [`Yes`,`No`,`Cannot say`] .\\n\\n\\n'\n",
    "              'Tweet:')\n",
    "\n",
    "# Function to parse the API response and return a dictionary\n",
    "def parse_api_response(response):\n",
    "    # Split the response by new lines\n",
    "    lines = response.split('\\n')\n",
    "    # Split each line by the first colon to separate the key and value, strip to remove leading/trailing whitespace\n",
    "    parsed = {line.split(':', 1)[0].strip(): line.split(':', 1)[1].strip() for line in lines if ':' in line}\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f6952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     tweet_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Shuffle user prompt tasks to avoid systematic order bias\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    tweet_text = row['text']\n",
    "    \n",
    "    # Shuffle user prompt tasks to avoid systematic order bias\n",
    "    user_prompt_sections = user_prompt.split('\\n\\n')\n",
    "    random.shuffle(user_prompt_sections)\n",
    "    shuffled_user_prompt = '\\n\\n'.join(user_prompt_sections)\n",
    "    user_prompt_text = shuffled_user_prompt + \"\\n\\n\" + tweet_text\n",
    "    \n",
    "    message_text = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_text}\n",
    "    ]\n",
    "\n",
    "    # Initialize a counter for the number of retries\n",
    "    retries = 0\n",
    "    max_retries = 30\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Calling the Azure OpenAI API\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                engine=\"GPT3\",\n",
    "                messages=message_text,\n",
    "                temperature=0,\n",
    "                api_key='ADD_API',  \n",
    "                stop=None\n",
    "            )\n",
    "            \n",
    "            # If the call was successful, break out of the loop\n",
    "            print('Completed call to API')\n",
    "            break\n",
    "        except openai.error.APIConnectionError as e:\n",
    "            print(f\"APIConnectionError: {e}. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "            retries += 1\n",
    "            continue\n",
    "        except openai.error.InvalidRequestError as e:\n",
    "            # This assumes that InvalidRequestError is the correct exception for content moderation issues\n",
    "            # You might need to adjust based on the actual exception and error message for content moderation\n",
    "            print(f\"Content moderation error: {e}. Skipping...\")\n",
    "            df.at[index, 'Offensive Assessment'] = 'Content Moderation'\n",
    "            break  # Skip further processing for this row\n",
    "\n",
    "    if retries >= max_retries:\n",
    "        # Handle the case where max retries have been reached\n",
    "        print(\"Max retries reached or content moderation issue detected. Moving to the next item.\")\n",
    "        continue\n",
    "\n",
    "    # Proceed with processing if the API call was successful and no moderation issues were encountered\n",
    "    if 'choices' in completion and len(completion['choices']) > 0:\n",
    "        response_content = completion['choices'][0]['message']['content']\n",
    "       \n",
    "        # Parse the API response\n",
    "        response_data = parse_api_response(response_content)\n",
    "        \n",
    "        # Update the DataFrame with the parsed data\n",
    "        for key, value in response_data.items():\n",
    "            if key not in df:\n",
    "                df[key] = pd.NA\n",
    "            df.at[index, key] = value\n",
    "        \n",
    "        # Print the updated DataFrame row to see the changes\n",
    "        print(df.loc[index])\n",
    "        print(df)\n",
    "        print(index)\n",
    "        \n",
    "        # Optionally, save the DataFrame periodically or after each update\n",
    "        df.to_csv('gpt_labels_random_1000.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17486f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
